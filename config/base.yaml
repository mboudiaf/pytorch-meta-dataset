TRAINING:
    print_freq: 100
    label_smoothing: 0.1
    num_updates: 100000
    ckpt_path: 'checkpoints'
    seed: 2021
    visdom_port: 0
    eval_freq: 500
    loss: 'xent'
    focal_gamma: 3

AUGMENTATIONS:
    beta: 1.0
    cutmix_prob: 1.0
    augmentation: 'none'

MODEL:
    arch: 'wideres2810'
    use_fc: True

OPTIM:
    gamma: 0.1
    lr: 0.1
    lr_stepsize: 30
    nesterov: False
    weight_decay: 0.0001
    optimizer_name: 'SGD'
    scheduler: 'multi_step'

DATA:
     # Data config
    image_size: 84  
    base_sources: ['ilsvrc_2012']  
    val_sources: ['ilsvrc_2012']  
    test_sources: ['ilsvrc_2012']  
    batch_size: 256  
    path: '/ssd/dataset/natural/converted'  
    split: 'train'  
    num_workers: 4  
    train_transforms: ['random_resized_crop', 'jitter', 'random_flip', 'to_tensor', 'normalize']  
    test_transforms: ['resize', 'center_crop', 'to_tensor', 'normalize']  
    num_unique_descriptions: 0
    shuffle: True
    loader_version: 'pytorch'

EVAL-GENERAL:
    eval_metrics: ['Acc']
    plot_freq: 10
    model_tag: 'best'
    eval_mode: 'test'
    val_episodes: 100
    val_batch_size: 1
    iter: 1
    extract_batch_size: 0  # set to >0 to batch feature extraction (save memory) at inference
    center_features: False


EVAL-VISU:
    res_path: 'results/'
    max_s_visu: 2
    max_q_visu: 3
    max_class_visu: 5
    visu: False
    visu_freq: 10

EVAL-EPISODES:
    num_ways: 10  
    num_support: 10  
    num_query: 15  
    min_ways: 5  
    max_ways_upper_bound: 50  
    max_num_query: 10  
    max_support_set_size: 500  
    max_support_size_contrib_per_class: 100  
    min_log_weight: -0.69314718055994529  
    max_log_weight: 0.69314718055994529  
    ignore_dag_ontology: False  
    ignore_bilevel_ontology: False  
    ignore_hierarchy_probability: 0  
    min_examples_in_class: 0  

METHOD:
    episodic_training: False

DISTRIBUTED:
    gpus: [0]